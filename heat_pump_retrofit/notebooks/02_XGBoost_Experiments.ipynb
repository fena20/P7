{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– XGBoost Model Experiments\n",
    "\n",
    "## Thermal Intensity Prediction Model\n",
    "\n",
    "**Author:** Fafa (GitHub: Fateme9977)  \n",
    "**Institution:** K. N. Toosi University of Technology\n",
    "\n",
    "---\n",
    "\n",
    "This notebook explores XGBoost hyperparameter tuning and model evaluation\n",
    "for predicting thermal intensity of gas-heated homes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Project paths\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'output'\n",
    "\n",
    "print(f'XGBoost version: {xgb.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(OUTPUT_DIR / '03_gas_heated_clean.csv')\n",
    "print(f'Loaded {len(df):,} households')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target variable\n",
    "print('Thermal Intensity distribution:')\n",
    "print(df['Thermal_Intensity_I'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features\n",
    "numeric_features = ['HDD65', 'A_heated', 'building_age', 'log_sqft']\n",
    "categorical_features = ['TYPEHUQ', 'YEARMADERANGE', 'DRAFTY', 'ADQINSUL', \n",
    "                        'TYPEGLASS', 'REGIONC', 'DIVISION', 'envelope_class', 'climate_zone']\n",
    "\n",
    "# Filter to available features\n",
    "numeric_features = [f for f in numeric_features if f in df.columns]\n",
    "categorical_features = [f for f in categorical_features if f in df.columns]\n",
    "\n",
    "print(f'Numeric features: {numeric_features}')\n",
    "print(f'Categorical features: {categorical_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrix\n",
    "all_features = numeric_features + categorical_features\n",
    "X = df[all_features].copy()\n",
    "y = df['Thermal_Intensity_I'].copy()\n",
    "\n",
    "# Remove missing targets\n",
    "valid_idx = y.notna()\n",
    "X = X[valid_idx]\n",
    "y = y[valid_idx]\n",
    "\n",
    "# Encode categoricals\n",
    "encoders = {}\n",
    "for col in categorical_features:\n",
    "    if X[col].dtype == 'object' or col in ['envelope_class', 'climate_zone']:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].fillna('missing').astype(str))\n",
    "        encoders[col] = le\n",
    "    else:\n",
    "        X[col] = X[col].fillna(-1)\n",
    "\n",
    "# Fill numeric NAs\n",
    "for col in numeric_features:\n",
    "    X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "print(f'Feature matrix shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Further split for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f'Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline XGBoost model\n",
    "baseline_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_train = baseline_model.predict(X_train)\n",
    "y_pred_val = baseline_model.predict(X_val)\n",
    "y_pred_test = baseline_model.predict(X_test)\n",
    "\n",
    "print('Baseline Model Performance:')\n",
    "print(f'  Train RÂ²: {r2_score(y_train, y_pred_train):.4f}')\n",
    "print(f'  Val RÂ²: {r2_score(y_val, y_pred_val):.4f}')\n",
    "print(f'  Test RÂ²: {r2_score(y_test, y_pred_test):.4f}')\n",
    "print(f'  Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.4f}')\n",
    "print(f'  Test MAE: {mean_absolute_error(y_test, y_pred_test):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'subsample': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Quick search (reduced grid for demonstration)\n",
    "quick_grid = {\n",
    "    'max_depth': [4, 6],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [100, 200],\n",
    "    'min_child_weight': [5, 10],\n",
    "}\n",
    "\n",
    "print('Running quick grid search...')\n",
    "grid_search = GridSearchCV(\n",
    "    xgb.XGBRegressor(random_state=42, n_jobs=-1),\n",
    "    quick_grid,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "print('\\nBest Parameters:')\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f'  {param}: {value}')\n",
    "\n",
    "print(f'\\nBest CV RÂ²: {grid_search.best_score_:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize grid search results\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Learning rate vs performance\n",
    "ax1 = axes[0]\n",
    "for depth in quick_grid['max_depth']:\n",
    "    mask = results_df['param_max_depth'] == depth\n",
    "    subset = results_df[mask].groupby('param_learning_rate')['mean_test_score'].mean()\n",
    "    ax1.plot(subset.index, subset.values, marker='o', label=f'max_depth={depth}')\n",
    "\n",
    "ax1.set_xlabel('Learning Rate', fontsize=12)\n",
    "ax1.set_ylabel('Mean CV RÂ²', fontsize=12)\n",
    "ax1.set_title('Learning Rate vs. Performance', fontsize=14)\n",
    "ax1.legend()\n",
    "\n",
    "# N estimators vs performance\n",
    "ax2 = axes[1]\n",
    "for depth in quick_grid['max_depth']:\n",
    "    mask = results_df['param_max_depth'] == depth\n",
    "    subset = results_df[mask].groupby('param_n_estimators')['mean_test_score'].mean()\n",
    "    ax2.plot(subset.index, subset.values, marker='s', label=f'max_depth={depth}')\n",
    "\n",
    "ax2.set_xlabel('Number of Estimators', fontsize=12)\n",
    "ax2.set_ylabel('Mean CV RÂ²', fontsize=12)\n",
    "ax2.set_title('N Estimators vs. Performance', fontsize=14)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tuned Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train tuned model with early stopping\n",
    "tuned_params = grid_search.best_params_.copy()\n",
    "tuned_params['n_estimators'] = 500\n",
    "tuned_params['early_stopping_rounds'] = 50\n",
    "\n",
    "tuned_model = xgb.XGBRegressor(\n",
    "    **{k: v for k, v in tuned_params.items() if k != 'early_stopping_rounds'},\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "tuned_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Final evaluation on test set\n",
    "y_pred_test = tuned_model.predict(X_test)\n",
    "\n",
    "print('Tuned Model Performance (Test Set):')\n",
    "print(f'  RÂ²: {r2_score(y_test, y_pred_test):.4f}')\n",
    "print(f'  RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.4f}')\n",
    "print(f'  MAE: {mean_absolute_error(y_test, y_pred_test):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Observed plot\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "ax.scatter(y_test, y_pred_test, alpha=0.5, s=20)\n",
    "\n",
    "# Add 45-degree line\n",
    "lims = [min(y_test.min(), y_pred_test.min()), max(y_test.max(), y_pred_test.max())]\n",
    "ax.plot(lims, lims, 'k--', alpha=0.75, linewidth=2)\n",
    "ax.set_xlim(lims)\n",
    "ax.set_ylim(lims)\n",
    "\n",
    "# Annotation\n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "ax.annotate(\n",
    "    f'RÂ² = {r2:.3f}\\nRMSE = {rmse:.2f}',\n",
    "    xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "    fontsize=12, verticalalignment='top',\n",
    "    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8)\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Observed Thermal Intensity', fontsize=12)\n",
    "ax.set_ylabel('Predicted Thermal Intensity', fontsize=12)\n",
    "ax.set_title('Predicted vs. Observed (Test Set)', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': tuned_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "top_n = min(15, len(importance_df))\n",
    "importance_df.head(top_n).plot.barh(\n",
    "    x='feature', y='importance', ax=ax, color='teal', legend=False\n",
    ")\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Feature Importance', fontsize=12)\n",
    "ax.set_title('Top Feature Importances', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nFeature Importance Ranking:')\n",
    "print(importance_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance by Subgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to test data\n",
    "test_results = df.loc[y_test.index].copy()\n",
    "test_results['predicted'] = y_pred_test\n",
    "test_results['error'] = y_test.values - y_pred_test\n",
    "test_results['abs_error'] = np.abs(test_results['error'])\n",
    "\n",
    "# Performance by envelope class\n",
    "if 'envelope_class' in test_results.columns:\n",
    "    print('Performance by Envelope Class:')\n",
    "    for env_class in ['poor', 'medium', 'good']:\n",
    "        mask = test_results['envelope_class'] == env_class\n",
    "        if mask.sum() > 0:\n",
    "            y_true = test_results.loc[mask, 'Thermal_Intensity_I']\n",
    "            y_pred = test_results.loc[mask, 'predicted']\n",
    "            print(f'  {env_class}: RÂ²={r2_score(y_true, y_pred):.3f}, '\n",
    "                  f'RMSE={np.sqrt(mean_squared_error(y_true, y_pred)):.3f}, '\n",
    "                  f'N={mask.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by climate zone\n",
    "if 'climate_zone' in test_results.columns:\n",
    "    print('\\nPerformance by Climate Zone:')\n",
    "    for climate in ['mild', 'mixed', 'cold']:\n",
    "        mask = test_results['climate_zone'] == climate\n",
    "        if mask.sum() > 0:\n",
    "            y_true = test_results.loc[mask, 'Thermal_Intensity_I']\n",
    "            y_pred = test_results.loc[mask, 'predicted']\n",
    "            print(f'  {climate}: RÂ²={r2_score(y_true, y_pred):.3f}, '\n",
    "                  f'RMSE={np.sqrt(mean_squared_error(y_true, y_pred)):.3f}, '\n",
    "                  f'N={mask.sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Residuals vs predicted\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(test_results['predicted'], test_results['error'], alpha=0.5, s=20)\n",
    "ax1.axhline(y=0, color='r', linestyle='--')\n",
    "ax1.set_xlabel('Predicted', fontsize=12)\n",
    "ax1.set_ylabel('Residual', fontsize=12)\n",
    "ax1.set_title('Residuals vs. Predicted', fontsize=14)\n",
    "\n",
    "# Histogram of residuals\n",
    "ax2 = axes[1]\n",
    "ax2.hist(test_results['error'], bins=50, color='steelblue', edgecolor='white')\n",
    "ax2.axvline(x=0, color='r', linestyle='--')\n",
    "ax2.set_xlabel('Residual', fontsize=12)\n",
    "ax2.set_ylabel('Count', fontsize=12)\n",
    "ax2.set_title('Distribution of Residuals', fontsize=14)\n",
    "\n",
    "# Q-Q plot\n",
    "ax3 = axes[2]\n",
    "from scipy import stats\n",
    "stats.probplot(test_results['error'], dist='norm', plot=ax3)\n",
    "ax3.set_title('Q-Q Plot of Residuals', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save model\n",
    "models_dir = OUTPUT_DIR / 'models'\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "joblib.dump(tuned_model, models_dir / 'xgboost_thermal_intensity.joblib')\n",
    "joblib.dump(encoders, models_dir / 'label_encoders.joblib')\n",
    "\n",
    "print(f'Model saved to {models_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Results:\n",
    "- Tuned XGBoost model achieves good predictive performance\n",
    "- Top features include draftiness, building age, floor area, and climate (HDD)\n",
    "- Model performs consistently across envelope classes and climate zones\n",
    "\n",
    "### Next Steps:\n",
    "- Run SHAP analysis for detailed interpretation\n",
    "- Use predictions in retrofit scenario analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
